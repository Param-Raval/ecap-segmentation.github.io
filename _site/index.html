<!DOCTYPE html>
<html lang="en-US">

  <head>
    <!-- General meta -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    

    
      <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>eCAP Segmentation | Blogpost on eCAP Segmentation and annotation tool</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="eCAP Segmentation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Blogpost on eCAP Segmentation and annotation tool" />
<meta property="og:description" content="Blogpost on eCAP Segmentation and annotation tool" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="eCAP Segmentation" />
<script type="application/ld+json">
{"@type":"WebSite","headline":"eCAP Segmentation","url":"http://localhost:4000/","name":"eCAP Segmentation","description":"Blogpost on eCAP Segmentation and annotation tool","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    

    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#242e2b"/>

    
      <link rel="stylesheet" href="/assets/styles.css">
    

    

    

    


    <!-- Overwrite this file with code you want before the closing head tag -->

  </head>

  <body class="layout-page  ">
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="height: 0; position: absolute">
  <symbol id="codepen" viewBox="0 0 16 16"><path d="M15.988 5.443c-.004-.02-.007-.04-.012-.058l-.01-.033c-.006-.017-.012-.034-.02-.05-.003-.012-.01-.023-.014-.034l-.023-.045-.02-.032-.03-.04-.024-.03c-.01-.013-.022-.026-.034-.038l-.027-.027-.04-.032-.03-.024-.012-.01L8.38.117c-.23-.155-.53-.155-.76 0L.305 4.99.296 5c-.012.007-.022.015-.032.023-.014.01-.027.02-.04.032l-.027.027-.034.037-.024.03-.03.04c-.006.012-.013.022-.02.033l-.023.045-.015.034c-.007.016-.012.033-.018.05l-.01.032c-.005.02-.01.038-.012.058l-.006.03C.002 5.5 0 5.53 0 5.56v4.875c0 .03.002.06.006.09l.007.03c.003.02.006.04.013.058l.01.033c.006.018.01.035.018.05l.015.033c.006.016.014.03.023.047l.02.03c.008.016.018.03.03.042.007.01.014.02.023.03.01.012.02.025.034.036.01.01.018.02.028.026l.04.033.03.023.01.01 7.31 4.876c.116.078.248.117.382.116.134 0 .266-.04.38-.116l7.314-4.875.01-.01c.012-.007.022-.015.032-.023.014-.01.027-.02.04-.032l.027-.027.034-.037.024-.03.03-.04.02-.032.023-.046.015-.033.018-.052.01-.033c.005-.02.01-.038.013-.058 0-.01.003-.02.004-.03.004-.03.006-.06.006-.09V5.564c0-.03-.002-.06-.006-.09l-.007-.03zM8 9.626L5.568 8 8 6.374 10.432 8 8 9.626zM7.312 5.18l-2.98 1.993-2.406-1.61 5.386-3.59v3.206zM3.095 8l-1.72 1.15v-2.3L3.095 8zm1.237.828l2.98 1.993v3.208l-5.386-3.59 2.406-1.61zm4.355 1.993l2.98-1.993 2.407 1.61-5.387 3.59v-3.206zM12.905 8l1.72-1.15v2.3L12.905 8zm-1.237-.827L8.688 5.18V1.97l5.386 3.59-2.406 1.61z" fill-rule="nonzero"/></symbol>
  <symbol id="dribbble" viewBox="0 0 16 16"><path d="M8 16c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zm6.747-6.905c-.234-.074-2.115-.635-4.257-.292.894 2.456 1.258 4.456 1.328 4.872 1.533-1.037 2.624-2.68 2.93-4.58zM10.67 14.3c-.102-.6-.5-2.688-1.46-5.18l-.044.014C5.312 10.477 3.93 13.15 3.806 13.4c1.158.905 2.614 1.444 4.194 1.444.947 0 1.85-.194 2.67-.543zm-7.747-1.72c.155-.266 2.03-3.37 5.555-4.51.09-.03.18-.056.27-.08-.173-.39-.36-.778-.555-1.16C4.78 7.85 1.47 7.807 1.17 7.8l-.003.208c0 1.755.665 3.358 1.756 4.57zM1.31 6.61c.307.005 3.122.017 6.318-.832-1.132-2.012-2.353-3.705-2.533-3.952-1.912.902-3.34 2.664-3.784 4.785zM6.4 1.368c.188.253 1.43 1.943 2.548 4 2.43-.91 3.46-2.293 3.582-2.468C11.323 1.827 9.736 1.176 8 1.176c-.55 0-1.087.066-1.6.19zm6.89 2.322c-.145.194-1.29 1.662-3.816 2.694.16.325.31.656.453.99.05.117.1.235.147.352 2.274-.286 4.533.172 4.758.22-.015-1.613-.59-3.094-1.543-4.257z"/></symbol>
  <symbol id="designernews" viewBox="0 0 16 16"><path d="M7.514 7.988c0-2.555-1.57-4.287-4.56-4.287H0v8.6h3.016c2.903 0 4.498-1.75 4.498-4.31zM5.37 8c0 1.844-.946 2.642-2.467 2.642H2.13V5.358h.773C4.36 5.358 5.37 6.193 5.37 8zM16 12.3V3.7h-1.98v4.81L10.853 3.7h-2.07v8.6h1.982V7.152l3.39 5.146H16z"/></symbol>
  <symbol id="facebook" viewBox="0 0 16 16"><path d="M15.117 0H.883C.395 0 0 .395 0 .883v14.234c0 .488.395.883.883.883h7.663V9.804H6.46V7.39h2.086V5.607c0-2.066 1.262-3.19 3.106-3.19.883 0 1.642.064 1.863.094v2.16h-1.28c-1 0-1.195.476-1.195 1.176v1.54h2.39l-.31 2.416h-2.08V16h4.077c.488 0 .883-.395.883-.883V.883C16 .395 15.605 0 15.117 0" fill-rule="nonzero"/></symbol>
  <symbol id="flickr" viewBox="0 0 16 16"><path d="M0 8c0 2.05 1.662 3.71 3.71 3.71 2.05 0 3.713-1.66 3.713-3.71S5.76 4.29 3.71 4.29C1.663 4.29 0 5.95 0 8zm8.577 0c0 2.05 1.662 3.71 3.712 3.71C14.337 11.71 16 10.05 16 8s-1.662-3.71-3.71-3.71c-2.05 0-3.713 1.66-3.713 3.71z"/></symbol>
  <symbol id="github" viewBox="0 0 16 16"><path d="M8 0C3.58 0 0 3.582 0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.083-.202-.358-1.015.077-2.117 0 0 .672-.215 2.2.82.638-.178 1.323-.266 2.003-.27.68.004 1.364.092 2.003.27 1.527-1.035 2.198-.82 2.198-.82.437 1.102.163 1.915.08 2.117.513.56.823 1.274.823 2.147 0 3.073-1.87 3.75-3.653 3.947.287.246.543.735.543 1.48 0 1.07-.01 1.933-.01 2.195 0 .215.144.463.55.385C13.71 14.53 16 11.534 16 8c0-4.418-3.582-8-8-8"/></symbol>
  <symbol id="hackernews" viewBox="0 0 16 16"><path d="M0 0v16h16V0H0zm8.92 8.96v3H7.25v-3l-2.75-5h1.96l1.66 3.48L9.7 3.96h1.88l-2.66 5z"/></symbol>
  <symbol id="instagram" viewBox="0 0 16 16"><path d="M8 0C5.827 0 5.555.01 4.702.048 3.85.088 3.27.222 2.76.42c-.526.204-.973.478-1.417.923-.445.444-.72.89-.923 1.417-.198.51-.333 1.09-.372 1.942C.008 5.555 0 5.827 0 8s.01 2.445.048 3.298c.04.852.174 1.433.372 1.942.204.526.478.973.923 1.417.444.445.89.72 1.417.923.51.198 1.09.333 1.942.372.853.04 1.125.048 3.298.048s2.445-.01 3.298-.048c.852-.04 1.433-.174 1.942-.372.526-.204.973-.478 1.417-.923.445-.444.72-.89.923-1.417.198-.51.333-1.09.372-1.942.04-.853.048-1.125.048-3.298s-.01-2.445-.048-3.298c-.04-.852-.174-1.433-.372-1.942-.204-.526-.478-.973-.923-1.417-.444-.445-.89-.72-1.417-.923-.51-.198-1.09-.333-1.942-.372C10.445.008 10.173 0 8 0zm0 1.44c2.136 0 2.39.01 3.233.048.78.036 1.203.166 1.485.276.374.145.64.318.92.598.28.28.453.546.598.92.11.282.24.705.276 1.485.038.844.047 1.097.047 3.233s-.01 2.39-.048 3.233c-.036.78-.166 1.203-.276 1.485-.145.374-.318.64-.598.92-.28.28-.546.453-.92.598-.282.11-.705.24-1.485.276-.844.038-1.097.047-3.233.047s-2.39-.01-3.233-.048c-.78-.036-1.203-.166-1.485-.276-.374-.145-.64-.318-.92-.598-.28-.28-.453-.546-.598-.92-.11-.282-.24-.705-.276-1.485C1.45 10.39 1.44 10.136 1.44 8s.01-2.39.048-3.233c.036-.78.166-1.203.276-1.485.145-.374.318-.64.598-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276C5.61 1.45 5.864 1.44 8 1.44zm0 2.452c-2.27 0-4.108 1.84-4.108 4.108 0 2.27 1.84 4.108 4.108 4.108 2.27 0 4.108-1.84 4.108-4.108 0-2.27-1.84-4.108-4.108-4.108zm0 6.775c-1.473 0-2.667-1.194-2.667-2.667 0-1.473 1.194-2.667 2.667-2.667 1.473 0 2.667 1.194 2.667 2.667 0 1.473-1.194 2.667-2.667 2.667zm5.23-6.937c0 .53-.43.96-.96.96s-.96-.43-.96-.96.43-.96.96-.96.96.43.96.96z"/></symbol>
  <symbol id="linkedin" viewBox="0 0 16 16"><path d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235 0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51V7.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4 0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762 0-1.376-.617-1.376-1.377 0-.758.614-1.375 1.376-1.375.76 0 1.376.617 1.376 1.375 0 .76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816 0H1.18C.528 0 0 .516 0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652 0 1.185-.516 1.185-1.153V1.153C16 .516 15.467 0 14.815 0z" fill-rule="nonzero"/></symbol>
  <symbol id="medium" viewBox="0 0 16 16"><path d="M11.824 12.628l-.276.45.798.398 2.744 1.372c.15.076.294.11.418.11.278 0 .467-.177.467-.492V5.883l-4.15 6.745zm4.096-8.67c-.004-.003 0-.01-.003-.012l-4.825-2.412c-.06-.03-.123-.038-.187-.044-.016 0-.03-.01-.047-.01-.184 0-.368.092-.467.254l-.24.39-.5.814-1.89 3.08 1.89 3.076.5.813.5.812.59.95 4.71-7.64c.02-.03.01-.06-.02-.08zm-6.27 7.045L7.17 6.97l-.295-.477-.294-.477-.25-.416v4.867l3.32 1.663.5.25.5.25-.5-.813-.5-.813zM.737 1.68L.59 1.608c-.085-.042-.166-.062-.24-.062-.206 0-.35.16-.35.427v10.162c0 .272.2.594.442.716l4.145 2.08c.107.06.208.08.3.08.257 0 .438-.2.438-.53V4.01c0-.02-.012-.04-.03-.047L.738 1.68z"/></symbol>
  <symbol id="pinterest" viewBox="0 0 16 16"><path d="M8 0C3.582 0 0 3.582 0 8c0 3.39 2.108 6.285 5.084 7.45-.07-.633-.133-1.604.028-2.295.146-.625.938-3.977.938-3.977s-.24-.48-.24-1.188c0-1.11.646-1.943 1.448-1.943.683 0 1.012.513 1.012 1.127 0 .687-.436 1.713-.662 2.664-.19.797.4 1.445 1.185 1.445 1.42 0 2.514-1.498 2.514-3.662 0-1.915-1.376-3.254-3.342-3.254-2.276 0-3.61 1.707-3.61 3.472 0 .687.263 1.424.593 1.825.066.08.075.15.057.23-.06.252-.196.796-.223.907-.035.146-.115.178-.268.107-.998-.465-1.624-1.926-1.624-3.1 0-2.524 1.834-4.84 5.287-4.84 2.774 0 4.932 1.977 4.932 4.62 0 2.757-1.74 4.977-4.153 4.977-.81 0-1.572-.422-1.833-.92l-.5 1.902c-.18.695-.667 1.566-.994 2.097.75.232 1.545.357 2.37.357 4.417 0 8-3.582 8-8s-3.583-8-8-8z" fill-rule="nonzero"/></symbol>
  <symbol id="rss" viewBox="0 0 16 16"><path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194 11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.985 2.194-2.196 2.194C.984 16 0 15.017 0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"/></symbol>
  <symbol id="reddit" viewBox="0 0 16 16"><path d="M1.473 9.368c-.04.185-.06.374-.06.566 0 2.3 2.94 4.173 6.554 4.173 3.613 0 6.553-1.872 6.553-4.173 0-.183-.02-.364-.055-.54l-.01-.022c-.013-.036-.02-.073-.02-.11-.2-.784-.745-1.497-1.533-2.072-.03-.01-.058-.026-.084-.047-.017-.013-.03-.028-.044-.043-1.198-.824-2.91-1.34-4.807-1.34-1.88 0-3.576.506-4.772 1.315-.01.012-.02.023-.033.033-.026.022-.056.04-.087.05-.805.576-1.364 1.293-1.572 2.086 0 .038-.01.077-.025.114l-.005.01zM8 13.003c-1.198 0-2.042-.26-2.58-.8-.116-.116-.116-.305 0-.422.117-.11.307-.11.424 0 .42.42 1.125.63 2.155.63 1.03 0 1.73-.2 2.15-.62.11-.11.3-.11.42 0 .11.12.11.31 0 .43-.54.54-1.38.8-2.58.8zM5.592 7.945c-.61 0-1.12.51-1.12 1.12 0 .608.51 1.102 1.12 1.102.61 0 1.103-.494 1.103-1.102 0-.61-.494-1.12-1.103-1.12zm4.83 0c-.61 0-1.12.51-1.12 1.12 0 .608.51 1.102 1.12 1.102.61 0 1.103-.494 1.103-1.102 0-.61-.494-1.12-1.103-1.12zM13.46 6.88c.693.556 1.202 1.216 1.462 1.94.3-.225.48-.578.48-.968 0-.67-.545-1.214-1.214-1.214-.267 0-.52.087-.728.243zM1.812 6.64c-.67 0-1.214.545-1.214 1.214 0 .363.16.7.43.927.268-.72.782-1.37 1.478-1.92-.202-.14-.443-.22-.694-.22zm6.155 8.067c-3.944 0-7.152-2.14-7.152-4.77 0-.183.016-.363.046-.54-.53-.33-.86-.91-.86-1.545 0-1 .82-1.812 1.82-1.812.45 0 .87.164 1.2.455 1.24-.796 2.91-1.297 4.75-1.33l1.21-3.69.27.063s.01 0 .01.002l2.82.663c.23-.533.76-.908 1.38-.908.82 0 1.49.67 1.49 1.492 0 .823-.67 1.492-1.49 1.492s-1.49-.67-1.49-1.49L9.4 2.18l-.98 2.99c1.77.07 3.37.57 4.57 1.35.33-.31.764-.48 1.225-.48 1 0 1.814.81 1.814 1.81 0 .66-.36 1.26-.92 1.58.02.17.04.33.04.5-.01 2.63-3.21 4.77-7.16 4.77zM13.43 1.893c-.494 0-.895.4-.895.894 0 .493.4.894.894.894.49 0 .89-.4.89-.89s-.4-.89-.9-.89z"/></symbol>
  <symbol id="skype" viewBox="0 0 16 16"><path d="M8.035 12.6c-2.685 0-3.885-1.322-3.885-2.313 0-.51.374-.865.89-.865 1.15 0 .85 1.653 2.995 1.653 1.096 0 1.703-.597 1.703-1.208 0-.368-.18-.775-.904-.954l-2.387-.597C4.524 7.833 4.175 6.79 4.175 5.812c0-2.034 1.91-2.798 3.704-2.798 1.65 0 3.6.916 3.6 2.136 0 .523-.452.827-.97.827-.98 0-.798-1.36-2.773-1.36-.98 0-1.523.444-1.523 1.08 0 .636.774.84 1.446.993l1.767.392c1.936.433 2.427 1.566 2.427 2.633 0 1.652-1.266 2.886-3.82 2.886m7.4-3.264l-.014.084-.028-.16c.015.024.028.05.042.076.082-.45.125-.912.125-1.373 0-1.023-.2-2.014-.595-2.948-.38-.902-.925-1.712-1.62-2.407-.692-.696-1.5-1.242-2.4-1.623C10.015.59 9.025.39 8.005.39c-.48 0-.963.045-1.43.135H6.57l.08.04-.16-.023.08-.016C5.927.183 5.205 0 4.472 0 3.278 0 2.155.466 1.31 1.313.465 2.16 0 3.286 0 4.483c0 .763.195 1.512.563 2.175l.013-.083.028.16c-.015-.026-.027-.052-.04-.077-.076.43-.115.867-.115 1.305 0 1.022.2 2.014.593 2.948.38.903.925 1.713 1.62 2.408.693.695 1.5 1.242 2.4 1.623.932.397 1.92.597 2.94.597.445 0 .89-.04 1.325-.118l-.077-.043.162.028-.084.014c.67.378 1.426.58 2.2.58 1.194 0 2.317-.466 3.162-1.313.845-.846 1.31-1.972 1.31-3.17 0-.765-.197-1.517-.566-2.18" fill-rule="nonzero"/></symbol>
  <symbol id="tumblr" viewBox="0 0 16 16"><path d="M9.708 16c-3.396 0-4.687-2.504-4.687-4.274V6.498H3.403V4.432C5.83 3.557 6.412 1.368 6.55.12c.01-.086.077-.12.115-.12H9.01v4.076h3.2v2.422H8.997v4.98c.01.667.25 1.58 1.472 1.58h.067c.424-.012.994-.136 1.29-.278l.77 2.283c-.288.424-1.594.916-2.77.936h-.12z" fill-rule="nonzero"/></symbol>
  <symbol id="twitch" viewBox="0 0 16 16"><g fill-rule="nonzero"><path d="M1.393 0L.35 2.783v11.13h3.824V16h2.088l2.085-2.088h3.13L15.65 9.74V0H1.394zm1.39 1.39H14.26v7.653l-2.435 2.435H8l-2.085 2.085v-2.085H2.783V1.39z"/><path d="M6.61 8.348H8V4.175H6.61v4.173zm3.824 0h1.39V4.175h-1.39v4.173z"/></g></symbol>
  <symbol id="twitter" viewBox="0 0 16 16"><path d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.375-1.337.648-2.085.795-.598-.638-1.45-1.036-2.396-1.036-1.812 0-3.282 1.468-3.282 3.28 0 .258.03.51.085.75C5.152 5.39 2.733 4.084 1.114 2.1.83 2.583.67 3.147.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21 0-.416-.02-.617-.058.418 1.304 1.63 2.253 3.067 2.28-1.124.88-2.54 1.404-4.077 1.404-.265 0-.526-.015-.783-.045 1.453.93 3.178 1.474 5.032 1.474 6.038 0 9.34-5 9.34-9.338 0-.143-.004-.284-.01-.425.64-.463 1.198-1.04 1.638-1.7z" fill-rule="nonzero"/></symbol>
  <symbol id="vimeo" viewBox="0 0 16 16"><path d="M15.992 4.275c-.07 1.56-1.16 3.697-3.263 6.41-2.176 2.832-4.017 4.248-5.522 4.248-.933 0-1.722-.862-2.367-2.588L3.55 7.6c-.48-1.724-.993-2.587-1.542-2.587-.12 0-.538.252-1.255.755L0 4.796C.79 4.1 1.568 3.406 2.335 2.71c1.053-.912 1.844-1.39 2.37-1.44 1.246-.12 2.012.733 2.3 2.56.31 1.97.526 3.194.647 3.673.36 1.634.754 2.45 1.185 2.45.335 0 .838-.53 1.51-1.59.67-1.06 1.028-1.866 1.076-2.42.096-.915-.263-1.374-1.077-1.374-.383 0-.778.087-1.185.262.788-2.58 2.29-3.834 4.508-3.762 1.644.048 2.42 1.116 2.324 3.205z" fill-rule="nonzero"/></symbol>
  <symbol id="youtube" viewBox="0 0 16 16"><path d="M0 7.345c0-1.294.16-2.59.16-2.59s.156-1.1.636-1.587c.608-.637 1.408-.617 1.764-.684C3.84 2.36 8 2.324 8 2.324s3.362.004 5.6.166c.314.038.996.04 1.604.678.48.486.636 1.588.636 1.588S16 6.05 16 7.346v1.258c0 1.296-.16 2.59-.16 2.59s-.156 1.102-.636 1.588c-.608.638-1.29.64-1.604.678-2.238.162-5.6.166-5.6.166s-4.16-.037-5.44-.16c-.356-.067-1.156-.047-1.764-.684-.48-.487-.636-1.587-.636-1.587S0 9.9 0 8.605v-1.26zm6.348 2.73V5.58l4.323 2.255-4.32 2.24h-.002z"/></symbol>
  <symbol id="link" viewBox="0 0 16 16"><path d="M5.86 12.7l-.81.8c-.7.7-1.84.7-2.54 0a1.75 1.75 0 0 1 0-2.5l2.98-2.96c.61-.61 1.77-1.52 2.62-.68a1 1 0 1 0 1.4-1.4c-1.44-1.43-3.57-1.17-5.42.67L1.1 9.6a3.72 3.72 0 0 0 0 5.32 3.78 3.78 0 0 0 5.34 0l.8-.8a1 1 0 1 0-1.39-1.42zm9.03-11.5c-1.55-1.53-3.7-1.6-5.14-.19l-1 1a1 1 0 1 0 1.39 1.41l1-1c.75-.74 1.72-.43 2.35.2a1.75 1.75 0 0 1 0 2.5l-3.17 3.15c-1.46 1.45-2.14.77-2.43.48a1 1 0 0 0-1.4 1.4c.67.67 1.43 1 2.23 1 .98 0 2.01-.5 3-1.47l3.17-3.15a3.72 3.72 0 0 0 0-5.32z"/></symbol>
  <symbol id="email" viewBox="0 0 16 11"><path fill-rule="evenodd" d="M1.33 0h13.34L8 5 1.33 0zM16 0v11H0V0l8 6 8-6z"/></symbol>
  <symbol id="nav" viewBox="0 0 16 11"><path d="M0 12h16v-2H0v2zm0-5h16V5H0v2zm0-7v2h16V0H0z"/></symbol>
</svg>


    <header class="header">
  <div class="container">
    

    
  <nav class="nav">
  <ul class="list list--nav">
    
      
    
      
        <li class="item  item--nav">

          

          
            <a href="/blog/"></a>
          
        </li>
      
    
      
        <li class="item  item--nav  item--current">

          

          
            <a href="/"></a>
          
        </li>
      
    
  </ul>
</nav>



<script type="text/javascript">
  // Get list and button
  const navList = document.querySelector('.header .list--nav')
  const button  = document.querySelector('.header .button--nav')

  // Hide nav and apply toggle
  const collapseNav = () => {
    if (document.body.clientWidth < 640) {
      navList.style.setProperty('--listHeight', `-${navList.offsetHeight}px`)
    } else {
      navList.removeAttribute('style')
    }

    button.onclick = () => {
      navList.style.setProperty('transition', `margin .1s`)
      if (navList.style.getPropertyValue('--listHeight')) {
        navList.style.removeProperty('--listHeight')
      } else {
        navList.style.setProperty('--listHeight', `-${navList.offsetHeight}px`)
      }
    }
  }

  collapseNav()

  // Check on resize if to collapse nav
  window.addEventListener('resize', () => {
    collapseNav()
  })
</script>

  </div>

  




  <div class="feature" style="background-image: url(https://param-raval.github.io/ecap-segmentation.github.io/assets/img/bios_bg.png)">
    <div class="container  typeset">
      <h2 id="automatic-fiber-type-segmentation-in-exogenous-peripheral-neural-signal">Automatic Fiber Type Segmentation in Exogenous Peripheral Neural Signal</h2>

    </div>
  </div>



</header>


<main class="main  container">

  <article class="article  article--page  content  typeset">

    <h1></h1>

    <h1 id="abstract">Abstract</h1>
<p>Vagus nerve stimulation (VNS) is a promising innovation in the treatment of chronic conditions already treating a large population in epilepsy, depression, and obesity. In its current clinical applications, patients receive a static dose uptitrated individually during clinic visits. It has been suggested that patient-responsive closed-loop therapies would lead to better outcomes and this area has seen a lot of research recently. Development of a closed-loop neuromodulation system
is challenging with the large amounts of data being transferred between the brain and the organs. Identifying different fibre types directly from nerve recordings is an important stepping stone towards closed-loop therapies. In the present work, we use machine learning to identify what fibres have been recruited in the vagus nerve following electrical stimulation. We propose novel ANN architectures trained on data from 6 subjects. The results are encouraging but we conclude that more data is needed to lower the error rate. Subsequently, we propose an annotation tool to visualise and annotate the segmented responses to create a richer dataset.</p>

<hr />

<ul>
  <li><a href="#abstract">Abstract</a></li>
  <li><a href="#neural-data-and-challenges">Neural Data and Challenges</a>
    <ul>
      <li><a href="#peripheral-nervous-system-and-vagus-nerve-stimulation">Peripheral nervous system and vagus nerve stimulation</a></li>
      <li><a href="#background-on-ecap-segmentation">Background on eCAP Segmentation</a>
        <ul>
          <li><a href="#dataset">Dataset</a></li>
        </ul>
      </li>
      <li><a href="#methods">Methods</a>
        <ul>
          <li><a href="#baseline">Baseline</a></li>
          <li><a href="#proposed-approaches-in-ecap-segmentation">Proposed Approaches in eCAP Segmentation</a></li>
          <li><a href="#bilstmsattention">BiLSTMs+Attention</a></li>
          <li><a href="#lstm-ed">LSTM-ED</a></li>
          <li><a href="#conv-ed">Conv-ED</a></li>
        </ul>
      </li>
      <li><a href="#experiment-setting">Experiment Setting</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#challenges-identified">Challenges Identified</a></li>
      <li><a href="#bootstrapping-analysis">Bootstrapping Analysis</a>
        <ul>
          <li><a href="#results-from-this-analysis">Results from this analysis</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#ecap-annotation-tool">eCAP Annotation Tool</a></li>
  <li><a href="#retraining-and-steps-towards-live-integration">Retraining and Steps Towards Live Integration</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#appendix-a">Appendix A</a></li>
</ul>

<hr />

<h1 id="neural-data-and-challenges">Neural Data and Challenges</h1>

<h3 id="peripheral-nervous-system-and-vagus-nerve-stimulation">Peripheral nervous system and vagus nerve stimulation</h3>
<p>The peripheral nervous system (PNS) serves as the conduit between the central nervous
system (CNS), comprising the brain and spinal cord, and the rest of the body. It encompasses
a vast network of nerves that extend throughout the body, connecting organs, muscles, and
tissues to the CNS. The PNS plays a crucial role in transmitting sensory information from
the environment to the brain, as well as coordinating motor responses that govern movement
and bodily functions.</p>

<p>One of the key components of the PNS is the vagus nerve, the longest cranial nerve in the body, which extends from the brainstem to various organs in the chest and abdomen. The vagus nerve is a major player in the parasympathetic nervous system (PSNS), regulating essential bodily functions such as heart rate, digestion, and respiratory rate.</p>

<p align="center">
<img src="http://localhost:4000/assets/img/vns.png" alt="image" />
</p>

<p><em><span id="fig1">Fig. 1</span>. Illustration of a traditional setup for vagus nerve stimulation consisting of a stimulator that passes electric stimulation through the leads and cuffs to the attached vagus nerve.</em></p>

<p>Vagus nerve stimulation (VNS) is a therapeutic technique that involves the delivery of electrical impulses to the vagus nerve. This process typically involves the surgical implantation of a small device, similar to a pacemaker, which is connected to the vagus nerve. The device generates controlled electrical pulses that travel along the vagus nerve, modulating its activity and influencing neural pathways involved in various physiological processes. By modulating the activity of the vagus nerve, VNS has been shown to influence neural pathways involved in mood regulation, seizure control, inflammation modulation, and autonomic function.</p>

<p>VNS has been utilised in clinical settings as an adjunctive therapy for various treatment resistant
neuropsychiatric and neurological conditions. For instance, VNS has been used as an adjunctive treatment for individuals with epilepsy with studies demonstrating reductions in seizure frequency and severity in patients receiving VNS therapy. Among other neurological disorders, VNS has emerged as a promising intervention for individuals with Treatment-Resistant Depression (TRD), a form of depression that does not respond to standard antidepressant medications or psychotherapy.</p>

<h2 id="background-on-ecap-segmentation">Background on eCAP Segmentation</h2>

<p><strong>eCAPs</strong>: At the core of VNS efficacy are the evoked compound action potentials (eCAPs), which provide crucial insights into the complex neurophysiological responses triggered by neural stimulation. The activations of different fibres in a nerve are evoked in the form of action potentials. The sum of synchronised action potentials from distinct axons is called a compound action potential (CAP). The neural response following VNS is thus characterised by eCAPs.</p>

<p>eCAPs represent the nerve responses to electrical stimulation including the activation of individual fibres that constitute the nerve. Understanding these eCAPs helps decipher the reaction of the nervous system to specific stimuli. This guides the parameter optimisation of the neuromodulation therapy to target specific physiological effects. To determine which vagal fibres produce the intended physiological effect, upon stimulation, different eCAPs must be elicited such that presence and absence of different fibre types can be controlled to examine the effects of activation pattern on physiology, see <a href="https://www.biorxiv.org/content/10.1101/2023.08.30.555487v1.full">Berthon (2023)</a>.</p>

<p><strong>Different fibre types</strong>: Fibre types in these nerves are classified based on the Erlanger-Gasser Classification into A, B and C types, with A having the lowest threshold for evoking eCAPs, followed by B and C. Further classification along with diameters and conduction velocities is shown in Table 1.</p>

<table>
  <thead>
    <tr>
      <th>Erlanger-Gasser Classification</th>
      <th>Diameter</th>
      <th>Conduction velocity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A-alpha</td>
      <td>13–20 μm</td>
      <td>80–120 m/s</td>
    </tr>
    <tr>
      <td>A-beta</td>
      <td>6–12 μm</td>
      <td>33–75 m/s</td>
    </tr>
    <tr>
      <td>A-gamma</td>
      <td>5–8 μm</td>
      <td>4–24 m/s</td>
    </tr>
    <tr>
      <td>A-delta</td>
      <td>1–5 μm</td>
      <td>3–30 m/s</td>
    </tr>
    <tr>
      <td>B</td>
      <td>&lt; 2 μm</td>
      <td>3-14 m/s</td>
    </tr>
    <tr>
      <td>C</td>
      <td>0.2-1.5 μm</td>
      <td>0.5–2.0 m/s</td>
    </tr>
  </tbody>
</table>

<p><em>Table 1: Erlanger-Gasser Classification of nerve fibres.</em></p>

<p>Firing of different types of fibres is linked to different physiological responses including laryngeal muscle, breathing, and heart rate responses. It is therefore highly desirable to be able to identify the recruited fibres following VNS. For example, the relationships between (i) changes in heart rate and B-fibre activations, as well as (ii) laryngeal muscle contractions (common VNS side-effect) and A-fibre activations have been studied in the literature.</p>

<p><strong>Modeling eCAPs</strong>: There are no known methods of automatically segmenting eCAPs following VNS and demonstrations in literature have relied on manual segmentation. This is a tedious process but more importantly the lack of automation precludes the use of segmented eCAPs in closed-loop therapy development (for example in optimising stimulation parameters for cardiac fibre recruitment). Moreover, with changing stimulation parameters such as current and pulse width, eCAP responses also change in their location of activation and shapes in the recordings. This becomes increasingly difficult to track manually.</p>

<p>Automated eCAP segmentation involves partitioning the neural response obtained from neural recordings as a time-series into distinct segments, each corresponding to specific fibre types activated by the stimulus. The neurogram samples are 1-channel 1D signals. Thus, in machine learning, this problem can be formulated as a 1D time-series segmentation problem.</p>

<p>Moreover, the stimulation experiment parameters have considerable effect on fibre activations. Across subjects it is seen that the B and A-delta fibres are activated more during higher current stimuli and activated earlier in stimuli with higher pulse widths. Additionally, the polarity of the stimulus and location of the cuff also affects the position of the activation. We design our models such that the features of current intensity (mA), stimulation pulse width (μs), and stimulation polarity (anodic/cathodic) can be fed in.</p>

<h3 id="dataset">Dataset</h3>
<p><strong>Neural interface</strong>: The vagus nerve is multi-fascicle in nature which poses a challenge to activate specific fibres and record their activation. The recorded intensity of a fibre activation varies around the nerve, therefore in this work we use proprietary, spatially-selective cuff electrodes that capture the responses from various positions on the nerve (see <a href="#fig2">Figure 2</a>). 
We collect neural responses to typical VNS waveforms: current (30-2500μA), frequency (1-20 Hz), pulse width (130-1000μs), and train duration (1-10s).</p>

<p><img src="http://localhost:4000/assets/img/cuff.png" alt="image" id="fig2" /></p>

<p><em>Fig. 2. Illustration of the three cuff electrode interface placed on the vagus nerve. Sourced from BIOS Health.</em></p>

<p>The responses were obtained from 6 porcine subjects. Typical neural responses to 17 different stimulation intensities are shown in <a href="#fig3">Figure 3</a>. Fibres of interest are labelled.</p>

<p>Every neurogram sample is a time-series of between 250 and 290 data points representing nerve activation in μV. To train sequential models, all samples were padded to 290 with trailing zeroes. The manually labelled bounds marking the start and end of different fibre activations were converted to a continuous label mask of the same length as the sample. Every data point corresponds to an integer label denoting the fibre type it belongs to. Points not belonging to one of the 4 fibre types of interest were encoded to a generic label “other”. The dataset contains 129,768 samples.</p>

<p><strong>Manual labelling</strong>: The data used in the current experiments was labelled manually by experts who observe the trends of activation of various fibres using a stacked plot similar to <a href="#fig3">Figure 3</a>. The subplots are arranged in increasing order of stimulation current intensity. Prior knowledge about the distance from the stimulation site to the recording site, the order in which various fibres are recruited and noting the gradual intensification of the activation of certain fibres help the expert in identifying them.</p>

<p>For instance, activations for fibres A-beta and A-gamma appear early at lower stimulation
currents than B-fibre. These are followed by a trailing laryngeal muscle artefact which
appears without conduction delay unlike other eCAPs. Across subjects, however, activations
of fibres differ in temporal location of activation as well as the intensity and shapes of the
activation spikes.</p>

<div style="display: flex; justify-content: center;">
<img src="http://localhost:4000/assets/img/stacked1.png" alt="image" id="fig3" />
</div>

<p><em>Fig. 3. Typical neural response after a stimulation (St, not shown) of pulse width 260μs and varying current. Note how activations become more prominent with increasing current, and how fibres like B and A-delta are activated only at higher currents.</em></p>

<p><br /></p>

<h2 id="methods">Methods</h2>

<h3 id="baseline">Baseline</h3>

<p>The simplest baseline averages the manually labelled start and end coordinates of the bounds for every fibre type across training subjects. All the test samples were assigned the same bounds for each fibre type and evaluated using F1-scores. This establishes a reasonable data-based baseline that achieves an average F1 score of x. However, as shown in <a href="#fig5">Figure 5</a>, we note that there is huge variability among subjects as well as different fibre types.</p>

<h3 id="proposed-approaches-in-ecap-segmentation">Proposed Approaches in eCAP Segmentation</h3>

<p>There exists no prior ANN work on eCAP segmentation, so we develop and compare 3 different models. Noting the success of image segmentation models using encoder-decoder architectures, we formulate our learning problem similarly where the model trains to convert a given sequence into another sequence of predicted labels for every point.</p>

<p>LSTM-based models have been successfully applied to sequence prediction and other time-series related tasks. As a starting point we take inspiration from existing work in ECG segmentation and begin with BiLSTM-based architectures. The BiLSTM architecture extends the capabilities of traditional LSTM networks by processing input sequences in both directions. Later, we move on to encoder-decoder based sequence-to-sequence architectures that are better suited for our problem.</p>

<p>Including multi–headed attention layers in these models allows them to focus on different parts of the input sequence simultaneously, enhancing its ability to capture relevant information.  This is particularly helpful in combining the additional numerical features.</p>

<p><br /></p>

<h3 id="bilstmsattention">BiLSTMs+Attention</h3>

<p><img src="http://localhost:4000/assets/img/segnet2.0.png" alt="image" id="fig4" /></p>

<p><em>Fig. 4. Architecture of the BiLSTMs model with an attention layer to weigh the numerical features. In order, there are 2 BiLSTM layers, 2 linear layers (32x1) with ReLU and 0.5 dropout, 1 linear layer for numerical features, self-attention with 8 heads, 0.1 dropout, and an output linear layer to predict the mask. “fc” represents fully-connected layers.</em></p>

<p>Together, BiLSTM layers enable the model to capture bidirectional context of the order
of occurrence of eCAPs, while attention mechanisms allow it to focus on relevant parts of
the input sequence and identify the fibre types.</p>

<p><br /></p>

<h3 id="lstm-ed">LSTM-ED</h3>

<p><img src="http://localhost:4000/assets/img/lstm_ae_model.png" alt="image" id="fig5" /></p>

<p><em>Fig. 5.  An encoder-decoder architecture with an added attention layer (4 attention heads with a dropout rate of 0.1)  to weigh the encoded representation with encoded numerical features. The encoder and decoder can be a series of BiLSTM layers (3 layers of BiLSTMs with a hidden size of 32 in LSTM-ED) or convolutional layers (Conv-ED). “fc” represents fully-connected layers.</em></p>

<p>The LSTM-ED model uses an encoder-decoder style architecture which is commonly used in image segmentation problems in computer vision.</p>

<p><strong>Encoder</strong>:  The encoder is similar to the BiLSTM+Attention model except being directly attached to the attention layer. The BiLSTMs in the encoder process the input eCAP data and extract high-level representations that capture temporal dynamics and patterns such as locations of fibre activations. The encoded output is as a condensed representation of the input which is weighted with the attention weights learnt from the encoded hidden state and encoded vector of numerical features.</p>

<p><strong>Decoder</strong>: The decoder attached immediately after the latter. This also has 3 layers of BiLSTMs with a hidden size of 32. The decoder is followed by a fully connected layer to produce the prediction mask. The decoder uses this learned representation of the eCAP data and transforms it to the corresponding output sequence of labels.</p>

<p><br /></p>

<h3 id="conv-ed">Conv-ED</h3>

<p>Conv-ED follows the same architecture as shown in <a href="#fig5">Figure 5</a> but with 1D convolutional layers in place of BiLSTM layers. In the encoder, there are 4 1D convolutional layers with 3x3 kernels each followed by a ReLU activation. In some layers, we also add dropout and 1D max pooling. The decoder has 4 transposed 1D convolutional layers with square kernels each followed by a ReLU and dropout except for the final convolutional layer which connects to a classification head.</p>

<p>Without the sequential and memory concepts of LSTM, the convolution layers learn local interactions between neighbouring data points in a time series. Convolutional layers automatically learn hierarchical features from the input and can capture patterns at different levels of abstraction. Moreover, the translation invariance of convolutions makes the model immune to time delays across samples, meaning they can detect patterns regardless of their position in the input sequence.</p>

<p>Table 2 notes the number of trainable parameters of each of these models.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: left"># of trainable parameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">BiLSTMs+Attn</td>
      <td style="text-align: left">80,229</td>
    </tr>
    <tr>
      <td style="text-align: left">LSTM-ED</td>
      <td style="text-align: left">38,949</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv-ED</td>
      <td style="text-align: left">63,333</td>
    </tr>
  </tbody>
</table>

<p><em>Table 2. Models and the number of parameters. Conv-ED is heavier given the multiple layers of convolutional layers in both encoder and decoder.</em></p>

<h2 id="experiment-setting">Experiment Setting</h2>

<p><strong>Normalisation</strong>: Since inter-subject variability is a well-recorded issue with eCAP data, a different strategy was used to normalise the training data. The normalisation parameters, mean and standard deviation, were computed for every subject instead for the entire training set. This makes sure for each subject that the individual data distributions and activation shapes are preserved and not biased by other subjects.</p>

<p><strong>Training split</strong>: Given the redundancy in a large proportion of samples within a subject, a random training, testing, and validation split is likely to cause samples similar to those “seen” in training to appear in testing. This does not simulate real-life settings where the model is expected to be utilised on a completely new subject. The splits were designed such that data from one subject remains wholly independent as a test set, one as validation, and the rest of the subjects form the training set.</p>

<p><strong>Training setting</strong>: The models were trained with a batch size of 128, for 40 epochs without early stopping using cross entropy loss, the Adam optimizer, and a learning rate of 1e-4.</p>

<p><img src="http://localhost:4000/assets/img/val_f1.png" alt="image" id="fig6" /></p>

<p><em>Fig. 6: Validation performance plateaus for the major fibre types around epoch 40 across subjects. Training beyond this either brings no change or reduces performance.</em></p>

<p><br /></p>

<h2 id="results">Results</h2>

<p><img src="http://localhost:4000/assets/img/good_example.png" alt="image" id="fig7" /></p>

<p><em>Fig. 7: A typical “good” example of eCAP segmentation from the test set with A-beta, A-gamma, and B fibre activations correctly captured (top: predicted segments, bottom: ground truth).</em></p>

<p><img src="http://localhost:4000/assets/img/bad_example.png" alt="image" id="fig8" /></p>

<p><em>Fig. 8:  A typical “bad” example of eCAP segmentation from the test set. B fibre activations are missed along with A-delta and the A-beta prediction overlaps A-gamma activations (top: predicted segments, bottom: ground truth). Mistakes of these types are commonly found in the incorrect predictions.</em></p>

<p>Table 3 and Table 4 give two levels of summarised results with <a href="#appendix-a">Appendix A</a> giving more details per subject. Table 3 shows the weighted averages of the F1 scores and the average macro  F1 scores of all fibre types (except “other”) considered over six test subjects. Given the class imbalance issues found in the dataset, these scores give a good overview to rank the models. Table 4 shows the average of F1 scores of 4 fibre types. We omit the “other” fibre type in the computation of these scores because it is the majority class in all the samples and the models can easily score well on it. Moreover, it is irrelevant to the downstream task of detecting various fibre types and given the good performance of the models on this class (check <a href="#appendix-a">Appendix A</a>), it can be safely ignored.</p>

<p>From Table 4, we can see that A-beta and A-gamma are easier to predict relative to other fibres. This can be explained from their consistent activations, in position and shape, in the majority of samples. On the contrary, the lack of consistency in B-fibre activations across subjects, brings the average down considerably. The poor performance in A-delta and B fibres can be explained by their muted activations and lack of proper representation in the data (discussed further later).</p>

<p>Overall, BiLSTM+Attn performs better than the rest and beats the baseline by a slim margin. Conv-ED fails to perform well whereas LSTM-ED does not give good enough improvement over the baseline.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Macro F1 score</th>
      <th>Weighted Average F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>0.37</td>
      <td>0.40</td>
    </tr>
    <tr>
      <td><strong>BiLSTM+Attn</strong></td>
      <td><strong>0.485</strong></td>
      <td><strong>0.498</strong></td>
    </tr>
    <tr>
      <td>LSTM-ED</td>
      <td>0.405</td>
      <td>0.485</td>
    </tr>
    <tr>
      <td>Conv-ED</td>
      <td>0.331</td>
      <td>0.391</td>
    </tr>
  </tbody>
</table>

<p><em>Table 3: Macro and Weighted Average Macro F1 scores per model</em></p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>A-beta</th>
      <th>A-gamma</th>
      <th>B</th>
      <th>A-delta</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>0.58</td>
      <td>0.53</td>
      <td>0.36</td>
      <td>0.03</td>
    </tr>
    <tr>
      <td><strong>BiLSTM+Attn</strong></td>
      <td><strong>0.635</strong></td>
      <td><strong>0.590</strong></td>
      <td><strong>0.442</strong></td>
      <td><strong>0.123</strong></td>
    </tr>
    <tr>
      <td>LSTM-ED</td>
      <td>0.601</td>
      <td>0.541</td>
      <td>0.433</td>
      <td>0.032</td>
    </tr>
    <tr>
      <td>Conv-ED</td>
      <td>0.520</td>
      <td>0.501</td>
      <td>0.297</td>
      <td>0.023</td>
    </tr>
  </tbody>
</table>

<p><em>Table 4: Average F1 scores per model per fibre type</em></p>

<h2 id="challenges-identified">Challenges Identified</h2>

<p>Comparing the four methods reported here, the performance improvements after using ML are not significant. However, the ML model offers adaptability to unknown subjects and a more reliable, data-driven approach to addressing the task. Yet, there are a number of challenges that restrain performance and generalisation across subjects.</p>

<table>
  <thead>
    <tr>
      <th>class</th>
      <th>point count</th>
      <th>point dist</th>
      <th>sample count</th>
      <th>sample dist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>other</td>
      <td>29672849</td>
      <td>0.788</td>
      <td>129768</td>
      <td>1.000</td>
    </tr>
    <tr>
      <td>B</td>
      <td>3761043</td>
      <td>0.100</td>
      <td>103415</td>
      <td>0.797</td>
    </tr>
    <tr>
      <td>A-gamma</td>
      <td>2144551</td>
      <td>0.057</td>
      <td>118816</td>
      <td>0.916</td>
    </tr>
    <tr>
      <td>A-beta</td>
      <td>1780301</td>
      <td>0.047</td>
      <td>129301</td>
      <td>0.996</td>
    </tr>
    <tr>
      <td>A-delta</td>
      <td>273976</td>
      <td>0.007</td>
      <td>16805</td>
      <td>0.130</td>
    </tr>
  </tbody>
</table>

<p><em>Table 5. A frequency distribution and statistics table highlighting the representation of different fibre types in the data. Point count and distribution show the number of data points that are labelled as the corresponding fibre type and their fraction relative to the rest of the points respectively. Similarly, sample count and distribution show the same statistics but on a sample level with 129,768 being the total number of samples in the dataset.</em></p>

<ul>
  <li><strong>Class imbalance</strong>: Table 5 shows various statistics about the frequency distribution of the classes in the dataset. It is evident from the point distribution column the diminished representation of fibre types of interest when compared to other fibres which take up the most space in the dataset. This explains the high performance of the latter class. However, A-gamma and A-beta have sufficient representation in the number of samples that contain them. This is not the case for A-delta which is the minority class on both fronts. Because of this, the performance for A-delta is egregiously low in all the experiments.</li>
</ul>

<p>This problem could not be resolved sufficiently well with weighted loss functions (like Focal loss or Dice loss), weighted resampling, data augmentation by rolling the time series, jittering, and linear modulation. Compared to our initial datasets (around 2,500 samples) where the samples contained pulses from only one recording channel (recording electrode) and the pulse trains were averaged, using entire pulse trains from all channels has boosted performance.</p>

<ul>
  <li><strong>Inter-subject variability</strong>: As seen in Figures in <a href="#appendix-a">Appendix A</a>, the inter quartile range (IQR) for every fibre type is widened by large variations in performance among different test subjects. Certain subjects like <em>IT6-A5</em> and <em>IT7-A1</em> consistently perform poorly across fibres and models. Inspecting visually, the shapes of the eCAPs in <em>IT7-A1</em> are quite different from the rest of the subjects. In <em>IT6-A5</em>, the samples are visually similar except for a muted peak in A-beta and the absence of a valley-like shape towards the beginning which is present in other subjects (see <a href="#appendix-a">Appendix A</a>).</li>
</ul>

<p>Such ambiguous differences among subjects have been difficult to explain and hinder model generalisation. Moreover, this also makes effective data augmentation challenging since the number of samples are large, uniformly varied representation of the classes is insufficient.</p>

<ul>
  <li><strong>Class representation</strong>: Table 5 shows how A-delta is the least prominent fibre within a sample and the number of samples that contain A-delta is just 13% of the entire set. In the presence of other prominent fibres, getting the model to get better at A-delta has been challenging.</li>
</ul>

<p>For B-fibre, despite having decent representation in the dataset, models fail to perform beyond a certain point. Subjects in both LSTM-ED and Conv-ED struggle to cross the 0.7 score while some remain around 0.1. Given the relatively mutated activations in B-fibre peaks, there are not sufficiently varied samples in quality and quantity in the dataset that the model can strongly fit to. Subjects in which B-fibre performs well have consistent and prominent activations that resemble other subjects. To replicate this in other subjects, a larger number of varied samples is necessary.</p>

<p><br /></p>

<h2 id="bootstrapping-analysis">Bootstrapping Analysis</h2>

<p>With the data challenges identified, acquisition of more and better data is required to drive this problem towards a solution. A bootstrapping analysis was conducted to confirm this intuition.</p>
<p align="center">
  <img src="http://localhost:4000/assets/img/bootstrapping.png" alt="image" id="fig9" />
</p>

<p><em>Fig. 9. Diagrammatic explanation of the bootstrapping process.</em></p>

<ul>
  <li>The aim was to validate that with greater number of subjects, better generalisation can be achieved.</li>
  <li>The setup involved the following procedure:
    <ul>
      <li>∀N ∈ 1,2,3,4, N randomly sampled subjects formed the training set, one subject from the rest for validation, and one for testing.</li>
      <li>The selected model was trained and the metrics logged.</li>
      <li>This process was repeated T = 3 times for every N.</li>
    </ul>
  </li>
  <li>LSTM-ED was used with 8 attention heads with the same cross-validated split strategy and training hyperparameters as described earlier.</li>
  <li>A larger model was chosen in order to minimise variability in results due to model selection and to retain focus on the trends instead of individual scores.</li>
</ul>

<h3 id="results-from-this-analysis">Results from this analysis</h3>

<p><a href="#fig10">Figure 10</a> shows the results for A-beta, A-gamma, and B fibres from the bootstrapping
analysis. They can be summarised as follows:</p>

<ul>
  <li>Scores for A-beta and A-gamma show a clear upward trend as the number of training subjects increases.</li>
  <li>Similarly, the IQR reduces greatly signifying the increase in prediction confidence across test subjects. This also shows that adding more varied subjects helps models to capture general trends better.</li>
  <li>The last chart shows the results for the B-fibre where the upward trend of improvement is still visible.</li>
  <li>The performance stagnation in a few test subjects is interesting where despite more data the model is not able to get equivalent gains.</li>
  <li>This further validates the B-fibre challenges shared earlier where the higher variety B-fibre activations in a small sample space obstructs performance.</li>
</ul>

<p><img src="http://localhost:4000/assets/img/btsp_abeta.png" alt="image" id="fig10" />
<img src="http://localhost:4000/assets/img/btsp_agamma.png" alt="image" />
<img src="http://localhost:4000/assets/img/btsp_b.png" alt="image" /></p>

<p><em>Fig. 10. Results from bootstrapping analysis of A-beta, A-gamma, and B-fibres.</em></p>

<hr />

<h1 id="ecap-annotation-tool">eCAP Annotation Tool</h1>
<p>With the conclusion that more data is required to improve the performance of the models, a web-based annotation tool was developed to facilitate the process of labelling eCAPs. The tool allows users to visualise the segmented eCAPs along with the bounds predicted from the segmentation model and edit these bounds for every sample. The annotated data can then be used to retrain the models and improve their performance.</p>

<p>Existing method of creating manual labels for eCAP neurograms involves looking at multiple ramp up plots for a subject and creating a fixed set of bounds for different fibre types. Issues with this include:</p>

<ul>
  <li>Only fixed constraints and conditions can be added manually for stim current, polarity, and pulse duration, and recording cuff pair among others.</li>
  <li>This process leads to creation of labels which are not precise enough for training a machine learning model.</li>
  <li>Labelling includes several erroneous labels for fibre types which are either not present or not recognized if present.</li>
</ul>

<p>However, with the eCAP annotation tool,</p>

<ul>
  <li>Annotators can visualise, create, and edit bounds for individual samples.</li>
  <li>Use an UI which enables interaction with the bounds with functionality to create, delete, and edit bound boxes on top of the visualised neurogram. This improves precision and correctness of the labels.</li>
  <li>With an integrated predictive model, the predicted bounds can be displayed reducing the annotation task to only correction.</li>
  <li>A new training set with precise labels can be used to retrain the predictive model to improve performance.</li>
  <li>The tool can exist offline independently and integration with a pipeline to annotate samples during a live trial can use retraining to improve model predictions during the session.</li>
</ul>

<p>Figure 11 shows a brief demonstration of the tool showing how a user can create, edit, and delete individual fibre bounds for a sample.</p>

<p><img src="http://localhost:4000/assets/vid/tool_demo_small.gif" alt="gif" /></p>

<p><em>Fig. 11. Demo of the annotation tool with the user editing, adding, and deleting bounds for a sample. Meta data for the sample is useful for the team to get better context of the sample while annotating.</em></p>

<h1 id="retraining-and-steps-towards-live-integration">Retraining and Steps Towards Live Integration</h1>

<p>The proposed models and annotation tool can be seen integrated with existing frameworks and trial workflows to give 1) live predictions during a trial to allow real-time optimisation of stimulation parameters, and 2) update training data with new predictions verified via the tool and retrain models to improve performance. Retraining can be a straightforward process where predictions from new subjects are edited and verified from the tool and added to the training set, which triggers retraining of the models. The new version of the model can be evaluated with a series of unit tests and deployed to replace the existing version.</p>

<h1 id="conclusion">Conclusion</h1>

<p>The proposed approaches demonstrate the effectiveness of machine learning models towards modelling and segmenting eCAPs towards the goal of developing closed-loop neuromodulation therapies. With our results and analysis, we show how more trials and data collection efforts can improve the performance and enable deployed, practical use of this system. With the proposed annotation tool and retraining workflow, we provide a framework for this purpose.</p>

<hr />

<h1 id="appendix-a">Appendix A</h1>

<p style="text-align: center;">Fixed bounds baseline (test F1 score)</p>
<p><img src="http://localhost:4000/assets/img/baseline.png" alt="image" id="fig12" /></p>

<p><em>Fig. 12. Baseline F1 scores on the test sets.</em></p>

<p style="text-align: center;">BiLSTM+Attention (test F1 score)</p>

<p><img src="http://localhost:4000/assets/img/segnetresult.png" alt="image" id="fig13" /></p>

<p><em>Fig. 13. F1 scores from BiLSTM+Attention on the test sets.</em></p>

<p style="text-align: center;">Conv-ED (test F1 score)</p>

<p><img src="http://localhost:4000/assets/img/convaeresult.png" alt="image" id="fig14" /></p>

<p><em>Fig. 14. F1 scores from the convolutional encoder-decoder on the test sets.</em></p>

<p style="text-align: center;">LSTM-ED (test F1 score)</p>

<p><img src="http://localhost:4000/assets/img/lstmaeresult.png" alt="image" id="fig15" /></p>

<p><em>Fig.15. F1 scores from the LSTM encoder-decoder on the test sets.</em></p>

<p><strong>Baseline</strong>: <a href="#fig12">Figure 12</a> shows that the consistency of A-beta and A-gamma in their frequency and location of appearance gives them fairly decent scores when predicted blindly. A-delta and B fibres leave significant room for improvement. It is also worth noting that the interquartile range (IQR) is decently sized despite significant inter-subject variability. The non-fibre label (“other”), being the majority class, is the easiest to predict even using a data agnostic method.</p>

<p><strong>BiLSTM+Attention</strong>: <a href="#fig13">Figure 13</a> shows that the model does not perform well enough. While producing gains in A-delta and B fibres for some subjects, overall many others have taken a hit. The IQR has also widened in 3 of the 5 labels with four subjects being out of the range in A-beta and B.</p>

<p><strong>Conv-ED</strong>: <a href="#fig14">Figure 14</a> shows while the improvements over BiLSTM+Attention are modest in A-beta and A-gamma, there is a notable jump in performance in the B-fibre compared to the baseline. Three of the six subjects perform over 0.4 while two have over 0.6 in their F1-scores. In A-beta and A-gamma, the averages are still close to the baseline but the IQR has reduced slightly.</p>

<p><strong>LSTM-ED</strong>: <a href="#fig15">Figure 15</a> reports that the average performance for A-beta, A-gamma, and B fibres has improved over the baseline, with a notable advance in the B-fibre. Four subjects give a score of 0.4 and above in the B-fibre while the IQR in other fibres has narrowed further.</p>


  </article>

  

</main>

<footer class="footer">
  <div class="container">
    <div class="copyright  typeset">
      <small class="small">&copy; eCAP Segmentation 2024</small>
    </div>

    
<nav class="nav  nav--footer">
  <ul class="list list--nav">
    

      

      <li class="item  item--nav">
        <a href="/#top">Back to top</a>
      </li>
    
  </ul>
</nav>


  </div>
</footer>



    <script type="text/javascript">
(() => {
  const registerServiceWorker = () => {
    if (!navigator.serviceWorker) {
      return;
    }

    navigator.serviceWorker
      .register("/sw.js")
      .then(registration => {
        console.log("Service Worker: registered");
      })
      .catch(err => {
        console.log("Service Worker: registration failed ", err);
      });
  };

  registerServiceWorker();
})();
</script>


    <!-- Overwrite this file with code you want before the closing body tag -->

  </body>

</html>
